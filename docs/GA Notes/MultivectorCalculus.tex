\chapter{Multivector Calculus}
In order to develop multivector Lagrangian and Hamiltonian methods we need to be able to take the derivative of a multivector
function with respect to another multivector.  One example of this are Lagrangians that are function of spinors (which are 
even multivectors) as in quantum field theory.  This chapter contains a brief description of the mechanics of multivector derivataives.


\section{New Multivector Operations}
Define the index $\ndx{i}{r} =\paren{i_{1},i_{2},\dots,i_{r}}$ where $r \le n$ the dimension of the vector space and $\f{P}{\ndx{i}{r}}$ is the union of $0$ and the
set of ordered tuples $\ndx{i}{r} =\paren{i_{1},i_{2},\dots,i_{r}}$ defined by
\be
	\f{P}{\ndx{i}{r}} \equiv \set{\begin{array}{c} \set{0\mbox{ if }r=0}\cup \\
	\set{\paren{i_{1},i_{2},\dots,i_{r}}\mbox{ such that }i_{1}<i_{2}<\dots<i_{r}\mbox{ and } 1\le i_{j}\le n
	            \mbox{ for }1\le j\le r}\end{array}}.
\ee
Essentially $\f{P}{\ndx{i}{r}}$ is an index set that enumerates the $r$-grade bases of the geometric algebra of an $n$-dimensional vector space
where $0\le r \le n$.  Then define the basis blades
\be
	e_{\ndx{i}{r}} \equiv e_{i_{1}}\W e_{i_{2}}\W\dots\W e_{i_{r}}
\ee 
where $e_{\ndx{i}{0}} = e_{0} = 1$ and
\be
	e^{\ndx{i}{r}} \equiv e^{i_{r}}\W e^{i_{r-1}}\W\dots\W e^{i_{1}}
\ee 
where $e^{\ndx{i}{0}} = e^{0} = 1$ and the $e_{i}$'s form a basis for the multivector space. With these definitions
\be
	\eu{i}{r}\cdot \ed{j}{r} = \delta^{\ndx{i}{r}}_{\ndx{j}{r}}.
\ee
The multivector $X$ can now be written as
\be
	X = \sum_{r=0}^{n}\hspace{4pt}\sum_{\ndx{i}{r}\in\f{P}{\ndx{i}{r}}}\hspace{-6pt}X^{\ndx{i}{r}}e_{\ndx{i}{r}} =
	    \sum_{r=0}^{n}\hspace{4pt}\sum_{\ndx{i}{r}\in\f{P}{\ndx{i}{r}}}\hspace{-6pt}X_{\ndx{i}{r}}e^{\ndx{i}{r}}.
\ee
We now generalize the Einstein summation convention so we can write $X = X^{\ndx{i}{r}}e_{\ndx{i}{r}}$.  Now it is clear that $X$ is a $2^{n}$
dimensional vector with components $X^{\ndx{i}{r}}$. For example for $n=3$
\begin{align}
	X &= \grd{X}{0}+\grd{X}{1}+\grd{X}{2}+\grd{X}{3} \\
	  &= X^{0}+X^{1}e_{1}+X^{2}e_{2}+X^{3}e_{3}+ \nonumber \\
	  &\hspace{20pt}X^{12}e_{1}\W e_{2}+X^{13}e_{1}\W e_{3}+X^{23}e_{2}\W e_{3}+X^{123}e_{1}\W e_{2}\W e_{3}.
\end{align}

From the properties of the dot product of blades developed in section~\ref{sect5_6_1} we have for $r>1$
\begin{align}
	e_{\ndx{i}{r}}\cdot e_{\ndx{j}{r}} &= \paren{e_{i_{1}}\W\dots\W e_{i_{r}}}\cdot\paren{e_{j_{1}}\W\dots\W e_{j_{r}}} \\
	                                   &= \delta_{\ndx{i}{r}\ndx{j}{r}}e_{\ndx{i}{r}}^{2}
\end{align}

Now define the scalar product of two multivectors $A$ and $B$ by
\be
	A*B \equiv \grd{AB}{}.
\ee
Then
\begin{align}
	\grd{A}{r}*\grd{B}{s} &= 0 \mbox{ if } r \ne s \\
	\grd{A}{r}*\grd{B}{r} &= \grd{A}{r}\cdot\grd{B}{r} = \grd{B}{r}*\grd{A}{r}  \mbox{ if } r \ne 0 \\
	\grd{A}{0}*\grd{B}{0} &= \grd{A}{0}\grd{B}{0} = \grd{A}{}\grd{B}{} \\
	A*B &= \sum_{r=0}^{n}\grd{A}{r}*B =  \sum_{r=0}^{n}\grd{A}{r}*\grd{B}{r} = \grd{A}{}\grd{B}{}+\sum_{r=1}^{n}\grd{A}{r}\cdot\grd{B} {r} 
\end{align}
by grade counting arguments and the orthogonality properties of basis blades of grade greater that 1.  Note that since 
$\grd{A}{r}$ is a sum of blades each defining a different subspace we have by equation~\ref{eq5_145} that the blades that
 compose $\grd{A}{r}$ are orthogonal to one another under the $*$ and $\cdot$ operations.  Also
\begin{align}
	A*B &= \grd{AB}{} = \grd{BA}{} = B*A \label{eq6_15}\\
	A*\paren{\alpha B+\beta C} &= \alpha A*B+\beta A*C \\
	A*B &= A^{\R}*B^{\R}. \label{eq6_17}
\end{align}
We also have
\be
	e^{\ndx{i}{r}}\cdot e_{\ndx{j}{r}} = e^{\ndx{i}{r}}* e_{\ndx{j}{r}} = \delta^{\ndx{i}{r}}_{\ndx{j}{r}} \mbox{ for } r>0.
\ee
We now use the scalar product to define the scalar magnitude $\abs{A}$ of a multivector $A$ by
\be
	\abs{A}^{2} \equiv A^{\R}*A = \sum_{r=0}^{n}\abs{\grd{A}{r}}^{2}
\ee
Now define a super metric tensor for the entire geometric algebra vector space by
\be
	G_{\ndx{i}{r}\ndx{j}{s}} \equiv \ed{i}{r}*\ed{j}{s} = \delta_{r,s}\set{\begin{array}{c}
		1 \mbox{ for }r=0 \\
		g_{\ndx{i}{1}\ndx{j}{1}} \mbox{ for }r=1 \\
		\delta_{\ndx{i}{r}\ndx{j}{r}}\paren{\ed{i}{r}}^{2}\mbox{ for }r>1
	\end{array}}{}
\ee
and
\be
	G^{\ndx{i}{r}\ndx{j}{s}} \equiv \eu{i}{r}*\eu{j}{s} = \delta^{r,s}\set{\begin{array}{c}
		1 \mbox{ for }r=0 \\
		g^{\ndx{i}{1}\ndx{j}{1}} \mbox{ for }r=1 \\
		\delta^{\ndx{i}{r}\ndx{j}{r}}\paren{\ed{i}{r}}^{-2}\mbox{ for }r>1
	\end{array}}{}
\ee
The super metric tensors have only diagonal entries except for $G_{\ndx{i}{1}\ndx{j}{1}}$ and $G^{\ndx{i}{1}\ndx{j}{1}}$.  Due to the block nature of
the $G$ tensors ($G_{\ndx{i}{r}\ndx{j}{s}} = G^{\ndx{i}{r}\ndx{j}{s}} = 0$ for $r\ne s$) we can write
\begin{align}
	\ed{j}{r} &= \eu{i}{r}\paren{\ed{i}{r}*\ed{j}{r}} = \eu{i}{r}\paren{\ed{j}{r}*\ed{i}{r}} \\
	\eu{j}{r} &= \ed{i}{r}\paren{\eu{i}{r}*\eu{j}{r}} = \ed{i}{r}\paren{\eu{j}{r}*\eu{i}{r}}.
\end{align} 
An additional relation that we need to prove is (no summation in this case)
\be
	\eu{i}{r}\ed{i}{r} = \eu{i}{r}\cdot\ed{i}{r} = 1.
\ee
First consider the case for $r=1$
\begin{align}
	\eu{i}{1}\ed{i}{1} &= e^{i_{1}}e_{i_{1}} =  e^{i_{1}}\cdot e_{i_{1}}+e^{i_{1}}\W e_{i_{1}} \nonumber \\
	                   &= 1+g^{j_{1}i_{1}}e_{j_{1}}\W  e_{i_{1}} = 1+\sum_{j_{1}<i_{1}} g^{j_{1}i_{1}}\paren{e_{j_{1}}\W  e_{i_{1}}+e_{i_{1}}\W  e_{j_{1}}}  = 1.                
\end{align}
Now consider the case $r>1$.  Since $\ed{i}{r}$ and $\eu{i}{r}$ are blades that define the same $r$-dimesional subspaces they can be written as the geometric
product of the same $r$ orthogonal vectors $o_{1},\dots,o_{r}$ to within a scale factor so that
\begin{align}
	\eu{i}{r}\ed{i}{r} &= \paren{e^{i_{r}}\W\dots\W e^{i_{1}}}\paren{e_{i_{1}}\W\dots\W e_{i_{r}}} \nonumber \\
	                   &= \alpha\beta\paren{o_{r}\dots o_{1}}\paren{o_{1}\dots o_{r}} = \alpha\beta o_{1}^{2}\dots o_{r}^{2} = \eu{i}{r}\cdot\ed{i}{r} =1 \label{eq6_26}
\end{align}
since by equation~\ref{eq6_26} $\eu{i}{r}\ed{i}{r} = \grd{\eu{i}{r}\ed{i}{r}}{}$ is a scalar.
\section{Derivatives With Respect to Multivectors}\label{MV_derivatives}
We start by discussing exactly what we mean we say $\f{F}{X}$ is a funtion of a multivector $X$.  First the domain and range of $\f{F}{X}$ are both 
multivectors in a $2^{n}$-dimensional vector space formed from the multivector space of the $n$-dimensional base vector space. Another way of stating this is
that the geometric algebra of the base vector space is generated by the $n$-grade pseudoscalar, $I_{n}$, of the base vector space .  The domain of $F$ is
the multivector space of the geometric algebra defined by the normalized pseudoscalar $I_{n}$ where $I_{n}^{2} = \pm 1$.  Thus we can
consider $X$ to be an element of the $2^{n}$ dimensional vector space defined by $I_{n}$. The partial dervatives of $\f{F}{X}$ are then the multivectors
$\pdiff{F}{X^{\ndx{i}{r}}}$ and there are $2^{n}$ of them. The definition of the multivector directional derivative $\partial_{X}$ is
\be
	\paren{A*\partial_{X}}F \equiv \lim_{h\rightarrow 0}\bfrac{\f{F}{X+hA}-\f{F}{X}}{h}.
\ee
Thus
\begin{align}
	\paren{A*\partial_{X}}F &= \lim_{h\rightarrow 0}\bfrac{\f{F}{X}+hA^{\ndx{i}{r}}\pdiff{F}{X^{\ndx{i}{r}}}-\f{F}{X}}{h} \\
	                &= A^{\ndx{i}{r}}\pdiff{F}{X^{\ndx{i}{r}}} \\
	                &= A^{\ndx{i}{r}}e_{\ndx{i}{r}}*e^{\ndx{j}{r}}\pdiff{F}{X^{\ndx{j}{r}}} \\
	                &= A*e^{\ndx{j}{r}}\pdiff{F}{X^{\ndx{j}{r}}} 
\end{align}
so that in terms of components
\be
	\partial_{X} = e^{\ndx{j}{r}}\pdiff{}{X^{\ndx{j}{r}}}.
\ee
Note that we have put parenthesis around $\paren{A*\partial_{X}}$ to remind ourselves (Doran and Lasenby do not do this) that $A*\partial_{X}$ is a scalar operater in
exactly the same way that $a\cdot\nabla$, $a\cdot\partial$, and $a\cdot D$ are scalar operators.  While not explicitly stated in D\&L or Hestenes, $*$ must have the
same precendence as $\cdot$ and higher than any of the other product of geometric algebra.  The multivector derivative $\partial_{X}$ is calculated by letting $A$ 
take on the values of the bases for the geometric algebra.

Another notation used for the multivector derivative is
\be
	\f{\underline{F}_{X}}{A} = \f{\underline{F}}{A} \equiv \paren{A*\partial_{X}}\f{F}{X}.
\ee
The form $\f{\underline{F}}{A}$ would be used if it is implicitely understood that the multivector derivative is to be taken with respect to $X$.

We can now define the adjoint of $\f{\underline{F}}{A}$ by
\be
	\f{\overline{F}}{B} \equiv \partial_{A}\grd{\f{\underline{F}}{A}B}{} \label{eq6_34a}
\ee
This make sense when we consider
\begin{align}
	\grd{A\f{\overline{F}}{B}}{} &= \grd{A\partial_{C}\grd{\f{\underline{F}}{C}B}{}}{} \nonumber \\
	                             &= \grd{A\partial_{C}}{}\grd{\f{\underline{F}}{C}B}{} \nonumber \\
	                             &= \paren{A*\partial_{C}}\grd{\f{\underline{F}}{C}B}{} \nonumber \\
	                             &= \lim_{h\rightarrow 0}\bfrac{\grd{\f{\underline{F}}{C+hA}B-\f{\underline{F}}{C}B}{}}{h} \nonumber \\
	                             &= \grd{\f{\underline{F}}{A}B}{} \nonumber \\
	       A*\f{\overline{F}}{B} &= \f{\underline{F}}{A}*B. \label{eq6_35a}
\end{align}
When $A$ and $B$ are vectors $a$ and $b$ and $\f{\underline{F}}{a}$ is a vector-valued linear function of $a$ we have
\begin{align}
	a*\f{\overline{F}}{b} &= \f{\underline{F}}{a}*b \nonumber \\
	a\cdot\f{\overline{F}}{b} &= \f{\underline{F}}{a}\cdot b \label{gen_adjoint}
\end{align}
which recovers the original definition of the adjoint.

\begin{center}
\fbox{\parbox{6in}{\emph{Note: Consider equation~\ref{eq6_35a} for the case that A and B are pure grade, but not the same grade.  For example let $B$ be a bivector, $\f{\overline{F}}{B}$ a vector, and $A$ a vector.  Then 
$A*\f{\overline{F}}{B} = A\cdot \f{\underline{F}}{B}$ is a scalar, but then $\f{\underline{F}}{A}$ must be a bivector for 
$\f{\underline{F}}{A}*B$ to be non-zero.  In general if $A$ and $\f{\overline{F}}{B}$ are pure grade they must be the same grade
then $\f{\underline{F}}{A}$ is the same pure grade as B and we can write $A\cdot \f{\overline{F}}{B} = B\cdot \f{\underline{F}}{A}$.}}}
\end{center}

Product rule -
\begin{align}
	\paren{A*\partial_{X}}\paren{FG} &= \lim_{h\rightarrow 0}\bfrac{\f{F}{X+hA}\f{G}{X+hA}-\f{F}{X}\f{G}{X}}{h} \nonumber \\
	                                 &= \lim_{h\rightarrow 0}\bfrac{\paren{\f{F}{X}+hA^{\ndx{i}{r}}\pdiff{F}{X^{\ndx{i}{r}}}}
	                                    \paren{\f{G}{X}+hA^{\ndx{j}{r}}\pdiff{G}{X^{\ndx{j}{r}}}}-\f{F}{X}\f{G}{X}}{h} \nonumber \\
	                                 &= A^{\ndx{i}{r}}\pdiff{F}{X^{\ndx{i}{r}}}G+A^{\ndx{j}{r}}F\pdiff{G}{X^{\ndx{j}{r}}} \nonumber \\
	                                 &= A^{\ndx{i}{r}}\paren{\pdiff{F}{X^{\ndx{i}{r}}}G+F\pdiff{G}{X^{\ndx{i}{r}}}} \nonumber \\
                                     &= A^{\ndx{i}{r}}\ed{i}{r}*\eu{j}{r}\dot{\pdiff{}{X^{\ndx{j}{r}}}}\paren{\dot{F}G+F\dot{G}} \nonumber \\
	                                 &= \paren{A*\dot{\partial}_{X}}\paren{\dot{F}G+F\dot{G}}
\end{align}
so that the product rule is
\be
	\partial_{X}\paren{FG} = \dot{\partial}_{X}\paren{\dot{F}G+F\dot{G}}.
\ee
Chain rule -
\begin{align}
	\paren{A*\partial_{X}}\f{F}{\f{G}{X}} &= \lim_{h\rightarrow 0}\bfrac{\f{F}{\f{G}{X+hA}}-\f{F}{\f{G}{X}}}{h} \nonumber \\
	                                      &= \lim_{h\rightarrow 0}\bfrac{\f{F}{\f{G}{X}+hA^{\ndx{i}{r}}\pdiff{G}{X^{\ndx{i}{r}}}}-\f{F}{\f{G}{X}}}{h} \nonumber \\
	                                      &= \lim_{h\rightarrow 0}\bfrac{\f{F}{\f{G}{X}}
	                                         +hA^{\ndx{i}{r}}\pdiff{G^{\ndx{j}{r}}}{X^{\ndx{i}{r}}}\pdiff{F}{G^{\ndx{j}{r}}}-\f{F}{\f{G}{X}}}{h} \nonumber \\
	                                      &= A^{\ndx{i}{r}}\pdiff{G^{\ndx{j}{r}}}{X^{\ndx{i}{r}}}\pdiff{F}{G^{\ndx{j}{r}}} \nonumber \\
	                                      &= A^{\ndx{i}{r}}\pdiff{G^{\ndx{j}{r}}}{X^{\ndx{i}{r}}}\ed{j}{r}*\eu{k}{r}\pdiff{F}{G^{\ndx{k}{r}}} \nonumber \\
	                                      &= \paren{A^{\ndx{i}{r}}\pdiff{G}{X^{\ndx{i}{r}}}}*\partial_{G}F \nonumber \\
	                                      &= \paren{\paren{A*\partial_{X}}G}*\partial_{G}F \label{eq_chainrule}
\end{align}
If $\f{g}{X}$ is a scalar function of a multivector $X$ and $\f{f}{g}$ is a scalar function of a scalar then
\be
	\paren{\paren{A*\partial_{X}}g}\deriv{f}{g} = \paren{A*\dot{\partial}_{X}}\dot{g}\deriv{f}{g}
\ee
and
\be
	\partial_{X}\f{f}{\f{g}{X}} = \paren{\partial_{X}g}\deriv{f}{g}.
\ee
One other general relationship of use is the evaluation of $\partial_{A}\paren{\paren{A*\partial_{X}}F}$
\begin{align}
	\paren{B*\partial_{A}}\paren{\paren{A*\partial_{X}}F} &= \lim_{h\rightarrow 0}\bfrac{\paren{\paren{\paren{A+hB}*\partial_{X}}F}-\paren{A*\partial_{X}}F}{h}.\nonumber \\
	                                                      &= \paren{B*\partial_{X}}F \label{eq6_40}                            
\end{align}
so that
\be
	 \partial_{A}\paren{\paren{A*\partial_{X}}F} = \partial_{X}F.\label{eq6_40a}
\ee
For simple derivatives we have the following -
\begin{enumerate}
\item $\partial_{X}\paren{A*X}$:
	\begin{align}
		\paren{B*\partial_{X}}\paren{A*X} =& \lim_{h\rightarrow 0}\bfrac{\grade{A\paren{X+hB}-AX}{}}{h} \nonumber \\
		                                  =& \grade{AB}{} = B*A \nonumber \\
		\partial_{X}\paren{A*X} =& A  \label{eq6_44a}                
	\end{align}
\item $\partial_{X}\paren{X*X^{\R}}$:
	\begin{align}
		\paren{A*\partial_{X}}\paren{X*X^{\R}} =& \lim_{h\rightarrow 0}\bfrac{\grade{\paren{X+hA}\paren{X+hA}^{\R}-XX^{\R}}{}}{h}
		                                          \nonumber \\
		                                  =& \grade{XA^{\R}+AX^{\R}}{} = X*A^{\R}+A*X^{\R} \nonumber \\
		                                  =& 2A*X^{\R} \label{6_45a} \\
		\partial_{X}\paren{X*X^{\R}} =&  2X^{\R} \label{eq6_46a} \\
		\partial_{X^{\R}}\paren{X*X^{\R}} =&  2X \label{eq6_47a}		              
	\end{align}
\item $\partial_{X}\paren{X*X}$:
	\begin{align}
		\paren{A*\partial_{X}}\paren{X*X} =& \lim_{h\rightarrow 0}\bfrac{\grade{\paren{X+hA}\paren{X+hA}-XX}{}}{h}
		                                          \nonumber \\
		                                  =& \grade{XA+AX}{} = X*A+A*X \nonumber \\
		                                  =& 2A*X \nonumber \\
		\partial_{X}\paren{X*X} =&  2X \label{eq6_48a}\\
		\partial_{X^{\R}}\paren{X^{\R}*X^{\R}} =&  2X^{\R} \label{eq6_49a}
	\end{align}
\item $\partial_{X}\paren{X^{\R}*X^{\R}}$:
	\begin{align}
		\paren{A*\partial_{X}}\paren{X^{\R}*X^{\R}} =& \lim_{h\rightarrow 0}\bfrac{\grade{\paren{X+hA}^{\R}
		                                            \paren{X+hA}^{\R}-X^{\R}X^{\R}}{}}{h} \nonumber \\
		                                  =& \grade{X^{\R}A^{\R}+A^{\R}X^{\R}}{} = X^{\R}*A^{\R}+A^{\R}*X^{\R} \nonumber \\
		                                  =& 2A*X \nonumber \\
		\partial_{X}\paren{X^{\R}*X^{\R}} =&  2X \label{eq6_50a}\\
		\partial_{X^{\R}}\paren{X*X} =&  2X^{\R} \label{eq6_51a}
	\end{align}
\item $\partial_{X}\paren{\abs{X}^{k}}$:
	\begin{align}
 \partial_{X}\paren{\abs{X}^{k}} &=  \partial_{X}\paren{\paren{\abs{X}^{2}}^{\frac{k}{2}}} \nonumber \\
                                 &= 2\bfrac{k}{2}\paren{\abs{X}^{2}}^{\frac{k}{2}-1}X^{\R} \nonumber \\
                                 &= k\abs{X}^{k-2}X^{\R}  \label{eq6_52a}
	\end{align}
	by the chain rule.
\end{enumerate}

\section{Calculus for Linear Functions}
Let $\f{\fbld}{a}$ be a linear function mapping the vector space onto itself and the $e_{i}$ the basis for the vector space then
\begin{align}
    \f{\fbld}{a} &= \f{\fbld}{a^{j}e_{j}} \nonumber \\
                 &= \f{\fbld}{e_{j}}a^{j} \nonumber \\
                 &= e^{i}\lp e_{i}\cdot\f{\fbld}{e_{j}}\rp a^{j}
\end{align}
The coefficient matrix for a linear function $\f{\fbld}{a}$ with respect to basis $e_{i}$ is defined as
\be
	f_{ij} \equiv e_{i}\cdot\f{\fbld}{e_{j}}
\ee
and
\be
     \f{\fbld}{a} = e^{i}f_{ij}a^{j}
\ee
Now consider the derivatives of the scalar $\f{\fbld}{b}\cdot c$ with respect to the $f_{ij}$
\begin{align}
	\partial_{f_{ij}}\f{\fbld}{b}\cdot c &= \partial_{f_{ij}}\paren{f_{lk}b^{k}c^{l}} \nonumber \\
	                                     &= \delta_{il}\delta_{jk}b^{k}c^{l} = b^{j}c^{i} \label{eq6_43}.
\end{align}
Multiplying equation~\ref{eq6_43} by $\paren{a\cdot e_{j}}e_{i}$ gives
\begin{align}
	\paren{a\cdot e_{j}}e_{i}\partial_{f_{ij}}\f{\fbld}{b}\cdot c &= \paren{a\cdot e_{j}}e_{i}b^{j}c^{i} \nonumber \\
	                                                              &= \paren{a\cdot e_{j}}cb^{j} \nonumber \\
	                                                              &= a_{j}b^{j}c = a_{j}e^{j}\cdot e_{k}b^{k}c = \paren{a\cdot b}c \label{eq6_44}
\end{align}
Since both $\f{\fbld}{b}\cdot c$ and $\paren{a\cdot b}c$ do not depend upon the selection of the basis $e_{i}$, then 
$\paren{a\cdot e_{j}}e_{i}\partial_{f_{ij}}$ also cannot depend upon the selection of the basis and we can define
\be
	\partial_{\f{\fbld}{a}} \equiv \paren{a\cdot e_{j}}e_{i} \partial_{f_{ij}} \label{eq6_45}
\ee
so that
\be
	\partial_{\f{\fbld}{a}} \paren{\f{\fbld}{b}\cdot c} = \paren{a\cdot b}c.
\ee
From equation~\ref{eq6_45} we see that $\partial_{\f{\fbld}{a}}$ is a vector operator and it obeys the product rule.

Now consider  $\partial_{\f{\fbld}{a}}\grd{\f{\fbld}{b\W c}B}{}$ where $B=b_{1}\W b_{2}$ is a bivector.  Then
\begin{align}
	\partial_{\f{\fbld}{a}}\grd{\f{\fbld}{b\W c}B}{} &= \partial_{\f{\fbld}{a}}\paren{\paren{\f{\fbld}{b}\W\f{\fbld}{c}}\cdot\paren{b_{1}\W b_{2}}} \nonumber \\
	        &= \dot{\partial}_{\f{\dot{\fbld}}{a}}\paren{\paren{\f{\dot{\fbld}}{b}\W\f{\fbld}{c}}\cdot\paren{b_{1}\W b_{2}}+
	           \paren{\f{\fbld}{b}\W\f{\dot{\fbld}}{c}}\cdot\paren{b_{1}\W b_{2}}} \nonumber \\
	        &= \dot{\partial}_{\f{\dot{\fbld}}{a}}\paren{\paren{\f{\dot{\fbld}}{b}\W\f{\fbld}{c}}\cdot\paren{b_{1}\W b_{2}}-
	           \paren{\f{\dot{\fbld}}{c}\W\f{\fbld}{b}}\cdot\paren{b_{1}\W b_{2}}},\label{eq6_47}
\end{align}
but by equation~\ref{eq465} (Appendix~\ref{app_A})
\be
	\paren{\f{\fbld}{b}\W\f{\fbld}{c}}\cdot\paren{b_{1}\W b_{2}} = \paren{\f{\fbld}{b}\cdot b_{2}}\paren{\f{\fbld}{c}\cdot b_{1}}-
	                                                               \paren{\f{\fbld}{b}\cdot b_{1}}\paren{\f{\fbld}{c}\cdot b_{2}}
\ee
so that (also using equation~\ref{eq447})
\begin{align}
	\dot{\partial}_{\f{\fbld}{a}}\paren{\paren{\f{\dot{\fbld}}{b}\W\f{\fbld}{c}}\cdot\paren{b_{1}\W b_{2}} }
	     &= \dot{\partial}_{\f{\fbld}{a}}\paren{\paren{\f{\dot{\fbld}}{b}\cdot b_{2}}\paren{\f{\fbld}{c}\cdot b_{1}}-
	                                                               \paren{\f{\dot{\fbld}}{b}\cdot b_{1}}\paren{\f{\fbld}{c}\cdot b_{2}}}\nonumber \\
         &= \paren{a\cdot b}\paren{\paren{\f{\fbld}{c}\cdot b_{1}}b_{2}-\paren{\f{\fbld}{c}\cdot b_{2}}b_{1}} \nonumber \\
         &= \paren{a\cdot b}\f{\fbld}{c}\cdot\paren{b_{1}\W b_{2}} = \paren{a\cdot b}\paren{\f{\fbld}{c}\cdot B}.
\end{align}
Thus equation~\ref{eq6_47} becomes
\begin{align}
	\partial_{\f{\fbld}{a}}\grd{\f{\fbld}{b\W c}B}{} &= \paren{a\cdot b}\paren{\f{\fbld}{c}\cdot B}-\paren{a\cdot c}\paren{\f{\fbld}{b}\cdot B} \nonumber \\
	                                                 &= \paren{\paren{a\cdot b}\f{\fbld}{c}-\paren{a\cdot c}\f{\fbld}{b}}\cdot B \nonumber \\
	                                                 &= \f{\fbld}{\paren{a\cdot b}c-\paren{a\cdot c}b}\cdot B \nonumber \\
	                                                 &= \f{\fbld}{a\cdot\paren{b\W c}}\cdot B.
\end{align}
In general if $A_{2}$ and $B_{2}$ are grade 2 multivectors then by linearity
\be
	\partial_{\f{\fbld}{a}}\grd{\f{\fbld}{A_{2}}B_{2}}{} = \f{\fbld}{a\cdot A_{2}}\cdot B_{2}.
\ee 
The general case can be proved using grade analysis. First consider the following (where the subscript indicates the grade of the multivector and 
$C$ is a general multivector)
\be
    \grd{A_{p}C}{} = \grd{A_{p}\paren{C_{0}+\cdots+C_{n}}}{} \label{eq6_51}
\ee
then the lowest grade of the general product term $A_{p}C_{q}$ is $\abs{p-q}$ which is only zero (a scalar) if $p=q$.  Thus 
\be
    \grd{A_{p}C}{} = \grd{A_{p}C_{p}}{} = A_{p}\cdot C_{p} \label{eq6_52}
\ee
Thus we may write by applying equation~\ref{eq6_52a} ($C$ is the multivector $\paren{\f{\fbld}{a_{2}}\dots\f{\fbld}{a_{r}}}B_{r}$)
\begin{align}
	\grd{\paren{\f{\fbld}{a_{1}}\W\dots\W\f{\fbld}{a_{r}}}B_{r}}{} &= \grd{\paren{\f{\fbld}{a_{1}}\dots\f{\fbld}{a_{r}}}{B_{r}}}{} \nonumber \\
	                                                               &= \f{\fbld}{a_{1}}\cdot\grd{\f{\fbld}{a_{2}}\dots\f{\fbld}{a_{r}}{B_{r}}}{1}
\end{align}
so that
\begin{align}
	\dot{\partial}_{\f{\fbld}{a}}\grd{\paren{\f{\dot{\fbld}}{a_{1}}\W\dots\W\f{\fbld}{a_{r}}}B_{r}}{} 
	        &= \dot{\partial}_{\f{\fbld}{a}}\paren{\f{\dot{\fbld}}{a_{1}}\cdot\grd{\f{\fbld}{a_{2}}\dots\f{\fbld}{a_{r}}{B_{r}}}{1}} \nonumber \\
			&= \paren{a\cdot a_{1}}\grd{\f{\fbld}{a_{2}}\dots\f{\fbld}{a_{r}}{B_{r}}}{1} \nonumber \\
			&= \grd{\paren{a\cdot a_{1}}\paren{\f{\fbld}{a_{2}}\dots\f{\fbld}{a_{r}}}{B_{r}}}{1} \nonumber \\
			&= \grd{\paren{\f{\fbld}{\paren{a\cdot a_{1}}a_{2}}\dots\f{\fbld}{a_{r}}}{B_{r}}}{1} \nonumber \\
			&= \grd{\paren{\f{\fbld}{\paren{a\cdot a_{1}}a_{2}}\W\dots\W\f{\fbld}{a_{r}}}{B_{r}}}{1} \nonumber \\
			&= \grd{\f{\fbld}{\paren{a\cdot a_{1}}a_{2}\W\dots\W a_{r}}B_{r}}{1}.
\end{align}
Thus (using equation~\ref{eqA_5} in Appendix~\ref{Appendix_A})
\begin{align}
	\partial_{\f{\fbld}{a}}\grd{\paren{\f{\fbld}{a_{1}}\W\dots\W\f{\fbld}{a_{r}}}B_{r}}{}
		& = \sum_{i=1}^{r}\onep{r+1}\grd{\f{\fbld}{\paren{a\cdot a_{i}}a_{1}\W\dots\W\breve{a}_{i}\W\dots\W a_{r}}B_{r}}{1} \nonumber \\
		& = \grd{\f{\fbld}{\paren{\sum_{i=1}^{r}\onep{r+1}a\cdot a_{i}}a_{1}\W\dots\W\breve{a}_{i}\W\dots\W a_{r}}B_{r}}{1} \nonumber \\
		& = \grd{\f{\fbld}{a\cdot\paren{a_{1}\W\dots\W a_{r}}}B_{r}}{1}.\label{eq6_55}
\end{align}
Using linearity the general case is
\be
	\partial_{\f{\fbld}{a}}\grd{\f{\fbld}{A}B}{} = \sum_{r}\grd{\f{\fbld}{a\cdot\grd{A}{r}}\grd{B}{r}}{1}.\label{eq6_56}
\ee
For a fixed $r$-grade multivector $A_{r}$ we can write
\begin{align}
	\grd{\f{\fbld}{A_{r}}\dot{X}_{r}}{}\dot{\partial}_{X_{r}} &= \grd{\f{\fbld}{A_{r}}\pdiff{X^{\ndx{i}{r}}}{X^{\ndx{j}{r}}}\ed{i}{r}}{}\eu{j}{r} \nonumber \\
	                                                          &= \grd{\f{\fbld}{A_{r}}\ed{i}{r}}{}\eu{i}{r} \nonumber \\
	                                                          &= \paren{\f{\fbld}{A_{r}}\cdot\ed{i}{r}}\eu{i}{r} = \f{\fbld}{A_{r}}. \label{eq6_57}
\end{align}
Applying equation~\ref{eq6_56} to equation~\ref{eq6_57} gives (let $\f{\fbld}{a\cdot A_{r}} = B_{i_{1}\dots i_{r-1}}e^{i_{r-1}}\W\dots\W e^{i_{1}}$ be a coordinant 
expansion for $\f{\fbld}{a\cdot A_{r}}$)
\begin{align}
	\partial_{\f{\fbld}{a}}\f{\fbld}{A_{r}} &= \partial_{\f{\fbld}{a}}\grd{\f{\fbld}{A_{r}}\dot{X}_{r}}{}\dot{\partial}_{X_{r}} \nonumber \\
	                                        &= \paren{\f{\fbld}{a\cdot A_{r}}\cdot\dot{X_{r}}}\dot{\partial}_{X_{r}} \nonumber \\
	                                        &= B_{i_{1}\dots i_{r-1}}\paren{e^{i_{r-1}}\W\dots\W e^{i_{1}}}\cdot\paren{e_{j_{1}}\W\dots\W e_{j_{r}}}
	                                           e^{j_{r}}\W\dots\W e^{j_{1}}. \label{eq6_58}
\end{align}
From what we know about subspaces the coefficients of $B_{i_{1}\dots i_{r-1}}$ are zero unless the $e_{j_{1}}\W\dots\W e_{j_{r}}$ contain the same vectors 
(possibly in a different order) as the $e^{i_{r-1}}\W\dots\W e^{i_{1}}$ plus one additional basis vector, $e_{j}$.  Thus (let $E^{r-1} = e^{i_{r-1}}\W\dots\W e^{i_{1}}$
and $E_{r-1} = e_{i_{r-1}}\W\dots\W e_{i_{1}}$)
\be\label{eq6_59}
	\paren{e^{i_{r-1}}\W\dots\W e^{i_{1}}}\cdot\paren{e_{j_{1}}\W\dots\W e_{j_{r}}}e^{j_{r}}\W\dots\W e^{j_{1}} = \paren{E^{r-1}\cdot\paren{E_{r-1}\W e_{j}}}
		\paren{e^{j}\W E^{r-1}}.
\ee
The l.h.s of equation~\ref{eq6_59} can also be put in the form of the r.h.s. since even if the order of the $e_{j_{k}}$'s are scrambled they can alway be put into
the same order as the $e_{i_{k}}$'s via transposition.  If the order of the $e^{j_{k}}$'s are made to be the reverse of the $e_{j_{k}}$'s any minus signs generated
will cancell out.  Also the $E_{r-1}$ and the $E^{r-1}$ can be exchanged since they are scalar multiples of one another.  The total number of possible $e_{j}$'s is 
$n-r+1$ where $n$ is the dimension of the vector space. Now reduce the scalar coefficient of the blade in equation~\ref{eq6_59}
\begin{align}
	\paren{E^{r-1}\cdot\paren{E_{r-1}\W e_{j}}} &= \grd{E^{r-1}\paren{E_{r-1}\W e_{j}}}{1} \nonumber \\
	                                            &= \grd{E^{r-1}\paren{E_{r-1}e_{j}-E_{r-1}\cdot e_{j}}}{1} \nonumber \\
	                                            &= e_{j}-\grd{E_{r-1}\paren{E^{r-1}\cdot e_{j}}}{1} \nonumber \\
	                                            &= e_{j}.
\end{align}
Now reduce
\begin{align}
	e_{j}\paren{e^{j}\W E^{r-1}} &= e_{j}\cdot\paren{e^{j}\W E^{r-1}}+e_{j}\W\paren{e^{j}\W E^{r-1}} \nonumber \\
	                             &= e_{j}\cdot\paren{e^{j}\W E^{r-1}} \nonumber \\
	                             &= \paren{e_{j}\cdot e^{j}}E^{r-1}
	                                +\sum_{l=1}^{r-1}\onep{l}\paren{e_{j}\cdot e^{i_{l}}}e^{i_{r-1}}\W\dots\W\breve{e}^{i_{l}}\W\dots\W e^{i_{1}}  \nonumber \\
	                             &= E^{r-1}.
\end{align}
Thus
\be
	\partial_{\f{\fbld}{a}}\f{\fbld}{A_{r}} = \paren{n-r+1}\f{\fbld}{a\cdot A_{r}}.
\ee
Now let $A_{r}=I$ then
\be
	\partial_{\f{\fbld}{a}} = \partial_{\f{\fbld}{a}}\f{\det}{\fbld}I = \f{\fbld}{a\cdot I},
\ee
but by equation~\ref{eq1_94} we have
\begin{align}
	\f{\det}{\fbld}\f{\fbld^{-1}}{a} &= I\f{\bar{\fbld}}{I^{-1}a} \nonumber \\
	                                     &= I^{-1}\f{\bar{\fbld}}{Ia} \nonumber \\
	\f{\det}{\fbld}I\f{\fbld^{-1}}{a} &= \f{\bar{\fbld}}{Ia} \nonumber \\
	\f{\det}{\fbld}\paren{\f{\fbld^{-1}}{a}}^{\R}I^{\R} &= \f{\bar{\fbld}}{\paren{Ia}^{\R}} \nonumber \\
	\f{\det}{\fbld}\f{\fbld^{-1}}{a}I^{\R} &= \f{\bar{\fbld}}{aI^{\R}} \nonumber \\
	\f{\det}{\fbld}\f{\fbld^{-1}}{a}I &= \f{\bar{\fbld}}{aI} \nonumber \\
	                                  &= \f{\bar{\fbld}}{a\cdot I} \nonumber \\
	\f{\det}{\fbld}\f{\bar{\fbld}^{-1}}{a}I &= \f{\fbld}{a\cdot I}                                  
\end{align}
or
\be
	\partial_{\f{\fbld}{a}}\f{\det}{\fbld} = \f{\det}{\fbld}\f{\bar{\fbld}^{-1}}{a}.
\ee
Equation~\ref{eq6_55} can also be used to calculate the functional derivative of the adjoint.  The result for $r>1$ is
\begin{align}
	\partial_{\f{\fbld}{a}}\f{\bar{\fbld}}{A_{r}} &= \partial_{\f{\fbld}{a}}\grd{\f{\fbld}{\dot{X}_{r}}A_{r}}{}\dot{\partial}_{X_{r}} \nonumber \\
	                                              &= \f{\fbld}{a\cdot \dot{X}_{r}}\cdot A_{r}\partial_{X_{r}}. \label{eq6_66}
\end{align}
Equation~\ref{eq6_66} cannot be used when $r=1$ since $\f{\fbld}{a\cdot X_{1}}$ is not defined. Let $A_{r}=b$ then using components we have
\begin{align}
	\partial_{\f{\fbld}{a}}\f{\bar{\fbld}}{b} &= a_{j}e_{i}\partial_{f_{ij}}e^{k}\bar{f}_{kl}b^{l} \nonumber \\
	                                          &= a_{j}e_{i}\partial_{f_{ij}}e^{k}f_{lk}b^{l} \nonumber \\
	                                          &= a_{j}e_{i}\delta_{l}^{i}\delta_{k}^{j}e^{k}b^{l} \nonumber \\
	                                          &= a_{j}e_{i}\e^{j}b^{i} = ba.
\end{align}
